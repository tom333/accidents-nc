# üö¶ Pr√©diction d'Accidents Routiers - Nouvelle-Cal√©donie

Syst√®me de pr√©diction des zones √† risque d'accidents routiers en Nouvelle-Cal√©donie (d√©partement 988) utilisant l'apprentissage automatique, les donn√©es OSM et les statistiques gouvernementales fran√ßaises.

## üìä Vue d'Ensemble

Ce projet impl√©mente un **classificateur binaire g√©ospatial-temporel** pour pr√©dire le risque d'accident par localisation et heure. Il combine :

- **Donn√©es officielles** : 5 ans d'accidents de data.gouv.fr (2019-2024)
- **R√©seau routier OSM** : 30+ communes de Nouvelle-Cal√©donie
- **Features temporelles** : Heure, jour de la semaine, mois, conditions m√©t√©o
- **√âchantillonnage n√©gatif intelligent** : Exclusion spatiale (300m) et distribution temporelle (85/15)
- **Orchestration Dagster** : Pipeline industrialis√© sur Kubernetes avec architecture m√©daillons
- **DuckLake** : Lakehouse moderne (DuckDB + PostgreSQL catalog + S3 storage)

### üéØ Performances du Mod√®le

**Meilleur mod√®le** : CatBoost (optimis√© avec Optuna - 50 trials)

| M√©trique | Accident | Pas Accident | Global |
|----------|----------|--------------|--------|
| **Recall** | **87.0%** | 99.7% | - |
| **Precision** | 97.9% | 98.3% | - |
| **F1-Score** | 0.92 | 0.99 | - |
| **Accuracy** | - | - | **98.2%** |

**R√©sultat cl√©** : D√©tecte **272/312 accidents r√©els** (87%), avec seulement 40 faux n√©gatifs et 6 faux positifs.

**Optimisation** : Hyperparam√®tres optimis√©s automatiquement via Optuna (50 essais, 74s d'entra√Ænement).

---

## üèóÔ∏è Architecture du Projet

### üê≥ Infrastructure (Production)

```
Kubernetes Cluster (microk8s)
‚îú‚îÄ‚îÄ Namespace: dagster
‚îÇ   ‚îú‚îÄ‚îÄ dagster-webserver       # UI Dagster (https://dagster.tgu.ovh)
‚îÇ   ‚îú‚îÄ‚îÄ dagster-daemon          # Scheduler & sensors
‚îÇ   ‚îî‚îÄ‚îÄ dagster-user-deployment # User code (assets bronze/silver/gold)
‚îú‚îÄ‚îÄ Namespace: datalab
‚îÇ   ‚îî‚îÄ‚îÄ postgresql              # DuckLake catalog (m√©tadonn√©es tables)
‚îî‚îÄ‚îÄ Namespace: ia-lab
    ‚îî‚îÄ‚îÄ rustfs-svc              # S3 compatible (https://rustfs.tgu.ovh)
```

### üìÇ Code Source

```
accidents/
‚îú‚îÄ‚îÄ dagster_accidents/          # Assets Dagster (orchestration)
‚îÇ   ‚îú‚îÄ‚îÄ assets_prod.py          # Bronze/Silver/Gold (sans training)
‚îÇ   ‚îú‚îÄ‚îÄ assets.py               # Version compl√®te avec gold_models
‚îÇ   ‚îî‚îÄ‚îÄ repository.py           # Definitions Dagster
‚îú‚îÄ‚îÄ pipeline/                   # Logique m√©tier (stage_ingest, stage_features, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Configuration DuckLake
‚îÇ   ‚îú‚îÄ‚îÄ stage_ingest.py         # ü•â Bronze: ingestion CSV
‚îÇ   ‚îú‚îÄ‚îÄ stage_features.py       # ü•à Silver: enrichissement + n√©gatifs
‚îÇ   ‚îú‚îÄ‚îÄ stage_datasets.py       # ü•á Gold: train/test splits
‚îÇ   ‚îî‚îÄ‚îÄ stage_modeling.py       # ü•á Gold: entra√Ænement mod√®les
‚îú‚îÄ‚îÄ k8s/dagster/                # Manifests Kubernetes
‚îÇ   ‚îú‚îÄ‚îÄ dagster-helm-app-updated.yaml  # ArgoCD Application
‚îÇ   ‚îú‚îÄ‚îÄ configmap-dagster-ducklake.yaml
‚îÇ   ‚îî‚îÄ‚îÄ DEPLOY_DAGSTER.md
‚îú‚îÄ‚îÄ Dockerfile.dagster          # Image user-code (1.12GB)
‚îú‚îÄ‚îÄ rebuild-and-deploy.sh       # Script rebuild/redeploy automatique
‚îú‚îÄ‚îÄ accident_fetch_data.py      # Notebook Marimo legacy (exploration)
‚îú‚îÄ‚îÄ predict_map.py              # Visualisation interactive (Marimo)
‚îú‚îÄ‚îÄ routes.nc                   # Cache OSM (~50MB, git-ignor√©)
‚îú‚îÄ‚îÄ INDUSTRIALISATION.md        # Guide Dagster + K8s
‚îî‚îÄ‚îÄ README.md                   # Cette documentation
```

### üèõÔ∏è Architecture M√©daillons (Lakehouse)

**DuckLake = DuckDB + PostgreSQL (catalog) + S3 (storage)**

```mermaid
graph LR
    A[CSV data.gouv.fr] -->|ingest_all| B[ü•â Bronze<br/>bronze.accidents_nc]
    B -->|build_feature_store| C[ü•à Silver<br/>silver.features]
    C -->|build_datasets| D[ü•á Gold<br/>gold.train/test]
    D -->|run_training| E[ü•á Gold<br/>gold_models + MLflow]
    
    B -.-> F[(PostgreSQL<br/>Catalog)]
    C -.-> F
    D -.-> F
    E -.-> F
    
    B -.-> G[(S3 RustFS<br/>Parquet)]
    C -.-> G
    D -.-> G
```

- **ü•â Bronze** : Donn√©es brutes (5 ans CSV, dep=988, datetime parsing)
- **ü•à Silver** : Features enrichies (OSM buffer 200m, n√©gatifs 22k ratio, temporelles)
- **ü•á Gold** : Datasets ML (80/20 split, encodage atm) + Mod√®les (CatBoost/LGBM/XGB)

### üîÑ Workflow de D√©veloppement

```bash
# 1. Modifier le code dans pipeline/ ou dagster_accidents/
vim pipeline/stage_ingest.py

# 2. Rebuild & redeploy automatique
./rebuild-and-deploy.sh

# 3. Mat√©rialiser via UI Dagster
# ‚Üí https://dagster.tgu.ovh > S√©lectionner asset > "Materialize"
```

---

## üöÄ Installation

### Pr√©requis

- Python 3.13+
- [uv](https://github.com/astral-sh/uv) (gestionnaire de paquets)
- Docker + Kubernetes (microk8s recommand√© pour local)
- Acc√®s √† un cluster PostgreSQL (DuckLake catalog)
- Acc√®s √† un stockage S3 compatible (DuckLake data storage)

### Installation des d√©pendances

```bash
# Cloner le projet
git clone <repo-url>
cd accidents

# Installer les d√©pendances (production uniquement)
uv sync

# Ou avec d√©pendances training (optuna, catboost, etc.)
uv sync --extra training

# Ou avec dev tools (marimo, dagster-webserver, pytest)
uv sync --extra dev

# Activer l'environnement virtuel
source .venv/bin/activate
```

### D√©pendances principales

**Core (production)** :
```toml
dagster>=1.8.0       # Orchestration
dagster-k8s          # Run launcher Kubernetes
dagster-postgres     # Storage backend
duckdb>=1.3.0        # Moteur SQL + DuckLake
geopandas, osmnx     # Analyse g√©ospatiale
scikit-learn         # Machine learning
polars, pandas       # Manipulation de donn√©es
pyarrow              # DuckDB ‚Üî Polars
boto3                # Client S3
```

**Training (optionnel)** :
```toml
catboost, lightgbm, xgboost  # Gradient boosting
optuna                        # Hyperparameter tuning
torch, pytorch-tabnet         # Deep learning
imblearn                      # Resampling
```

**Dev (optionnel)** :
```toml
marimo[recommended]   # Notebooks r√©actifs (exploration)
dagster-webserver     # UI locale
pytest                # Tests
```

---

## üìö Utilisation

### üîÅ Nouveau pipeline modulaire (DuckDB)

Les 4 notebooks Marimo du dossier `notebooks/` fonctionnent comme une cha√Æne : chacun consomme les tables DuckDB cr√©√©es par le pr√©c√©dent avant d'√©crire ses propres outputs. Toutes les donn√©es transitent par `data/accidents_pipeline.duckdb`.

| √âtape | Notebook | Tables √©crites | Description |
|-------|----------|----------------|-------------|
| 1 | `01_ingest_raw.py` | `raw.caracteristiques`, `raw.usagers`, `raw.accidents_nc` | Ingestion CSV/URLs et normalisation des colonnes cl√©s (datetime, lat/lon, atm). |
| 2 | `02_enrich_features.py` | `features.full_dataset` | G√©n√©ration des n√©gatifs spatialement s√ªrs, rattachement des features OSM/densit√©, ajout des features temporelles avanc√©es. |
| 3 | `03_prepare_datasets.py` | `datasets.train`, `datasets.test`, `datasets.feature_metadata` | Encodage `atm`, drop NA, split stratifi√©, sauvegarde `atm_encoder.pkl` et `features.pkl`. |
| 4 | `04_model_training.py` | Logs MLflow + artefacts `.pkl` | Tuning Optuna (CatBoost/LGBM/XGBoost) + ensembles TabNet/MLP/logistic, export `accident_model.pkl`. |

Pour ex√©cuter l‚Äôensemble :

```bash
marimo run notebooks/01_ingest_raw.py
marimo run notebooks/02_enrich_features.py
marimo run notebooks/03_prepare_datasets.py
marimo run notebooks/04_model_training.py
```

Chaque notebook peut aussi √™tre ouvert en mode interactif (`marimo edit ...`) pour ajuster les param√®tres et visualiser les outputs interm√©diaires (comptes DuckDB, previews, etc.).

### 1Ô∏è‚É£ Entra√Ænement du Mod√®le

> üí° **Recommand√©** : ex√©cuter les 4 notebooks modulaires (`01_` ‚Üí `04_`) pour b√©n√©ficier du stockage DuckDB et de la reprise par √©tape.
>
> Le notebook ci-dessous reste disponible pour des it√©rations rapides mais sera progressivement retir√©.

**Notebook Marimo (legacy)** : `accident_fetch_data.py`

```bash
marimo edit accident_fetch_data.py
```

**Pipeline complet** (14 cellules) :

1. **Ingestion** : Fusion 6 CSV data.gouv.fr via DuckDB
2. **Nettoyage** : Parsing dates fran√ßaises, filtrage coordonn√©es
3. **R√©seau OSM** : T√©l√©chargement 30+ communes, buffer 200m
4. **Grille spatiale** : R√©solution 0.02¬∞ (~2.2km), spatial join
5. **√âchantillonnage n√©gatif** :
   - Exclusion spatiale 300m autour accidents
   - Distribution temporelle 85% heures √† risque / 15% al√©atoire
   - Ratio 2:1 (n√©gatifs/positifs)
6. **Features engineering** : 
   - **6 features de base** : latitude, longitude, hour, dayofweek, month, atm
   - **18 features enrichies** :
     - Interactions spatio-temporelles (lat√óhour, lon√ódayofweek, etc.)
     - Attributs OSM (type de route, vitesse limite)
     - M√©triques de densit√© et proximit√©
     - Encodage cyclique temporel (sin/cos)
     - Indicateurs temporels avanc√©s (jours f√©ri√©s, vacances)
   - **Total : 24 features**
7. **Optimisation hyperparam√®tres** :
   - Framework **Optuna** avec MedianPruner
   - 50 essais par algorithme (CatBoost, LightGBM, XGBoost)
   - M√©trique : **Recall** (priorit√© d√©tection accidents)
   - S√©lection automatique du meilleur mod√®le
8. **√âvaluation** : Rapport classification, courbe ROC, importance features
9. **Export** : Mod√®les sauvegard√©s en `.pkl`

**Configuration optimis√©e** :

```python
CONFIG = {
    'n_negative_samples_ratio': 22000,   # Ratio r√©aliste bas√© sur taux d'accidents r√©el
    'buffer_meters': 200,
    'grid_step': 0.02,
    'accident_exclusion_buffer_km': 0.3, # 300m au lieu de 500m
    'temporal_risk_ratio': 0.85          # 85% heures √† risque
}
```

### üó∫Ô∏è Visualisation Interactive

**Notebook Marimo** : `predict_map.py`

```bash
marimo edit predict_map.py
```

**Interface utilisateur** :

- üìÖ **S√©lecteur de date** : N'importe quelle date
- üéØ **Mode de s√©lection** :
  - **Top N** (recommand√©) : Affiche les N points les plus dangereux par heure (1-10, d√©faut=3)
  - **Seuil** : Probabilit√© minimale (50-95%, d√©faut=70%)
- üå¶Ô∏è **Conditions m√©t√©o** : Normal, Pluie l√©g√®re, Pluie forte, Brouillard

**Carte Folium interactive** :

- Marqueurs color√©s par risque (rouge ‚â•80%, orange 60-80%, jaune <60%)
- Filtres par heure (panneau de contr√¥le)
- Popups avec d√©tails (heure, probabilit√©)
- Statistiques dynamiques (nombre de points, probabilit√©s min/max/moyenne)

**Outputs** :

- Carte interactive avec 24 couches (1 par heure)
- Tableau r√©capitulatif par heure
- R√©sum√© global (heure la plus dangereuse, risques moyen/max)

### ü§ñ Pr√©dictions Automatiques (√Ä faire)

**Script Python** : `predict_daily.py` (n√©cessite adaptation pour DuckLake)

```bash
# Pr√©dictions pour demain (conditions normales)
python predict_daily.py

# Date sp√©cifique
python predict_daily.py --date 2026-01-25

# Avec conditions m√©t√©o
python predict_daily.py --atm 2  # 1=Normal, 2=Pluie l√©g√®re, 3=Pluie forte, 5=Brouillard

# Base DuckDB personnalis√©e
python predict_daily.py --db custom.duckdb
```

**Pipeline automatique** :

1. Charge mod√®le + grille routi√®re (~1500 points)
2. G√©n√®re **24h √ó 1500 = 36,000 pr√©dictions**
3. Stocke dans DuckDB avec index optimis√©s
4. Affiche statistiques (risque moyen, points critiques)

**Automatisation cron** (23h chaque jour) :

```bash
crontab -e
# Ajouter :
0 23 * * * cd /path/to/accidents && python predict_daily.py >> predict.log 2>&1
```

### üîç Consultation des Donn√©es DuckLake

**DuckDB CLI** (voir [QUERIES.md](QUERIES.md) pour plus d'exemples) :

```bash
# Se connecter au catalog DuckLake (depuis un pod k8s)
kubectl exec -it -n dagster deployment/dagster-user-deployment-accidents -- \
  duckdb -c "ATTACH 'postgres://user:password@postgresql.datalab:5432/data' AS ducklake (TYPE postgres); SELECT * FROM ducklake.bronze.accidents_nc LIMIT 10;"

# Ou depuis un notebook Marimo local
marimo edit accident_fetch_data.py
# Dans le notebook:
import duckdb
from pipeline.config import ensure_connection
conn = ensure_connection()
conn.execute("SELECT * FROM bronze.accidents_nc LIMIT 10").pl()
```

**Requ√™tes utiles** :

```sql
-- Compter les accidents par ann√©e
SELECT EXTRACT(YEAR FROM event_time) as year, COUNT(*) as nb_accidents
FROM bronze.accidents_nc
GROUP BY year
ORDER BY year;

-- Top 10 features les plus importantes
SELECT * FROM silver.features WHERE target = 1 LIMIT 10;

-- Distribution train/test
SELECT 'train' as split, COUNT(*) as rows FROM gold.train
UNION ALL
SELECT 'test' as split, COUNT(*) as rows FROM gold.test;
```

**Stockage S3** :

```bash
# Lister les fichiers Parquet sur RustFS
aws --endpoint-url=https://rustfs.tgu.ovh s3 ls s3://accidents-bucket/ducklake/ --recursive

# T√©l√©charger une table localement
aws --endpoint-url=https://rustfs.tgu.ovh s3 cp \
  s3://accidents-bucket/ducklake/bronze/accidents_nc/ . --recursive
```

---

## üìê Sch√©ma de Donn√©es

### DuckLake (PostgreSQL Catalog + S3 Parquet Storage)

**Catalog PostgreSQL** : `postgresql.datalab.svc.cluster.local:5432/data`  
**Storage S3** : `s3://accidents-bucket/ducklake/` (RustFS)

#### ü•â Schema Bronze

```sql
-- Table principale: accidents New Caledonia
CREATE TABLE bronze.accidents_nc (
    Num_Acc VARCHAR,              -- ID accident (cl√©)
    event_time TIMESTAMP,         -- Datetime pars√© (format fran√ßais)
    latitude DOUBLE,              -- Coordonn√©e WGS84
    longitude DOUBLE,             -- Coordonn√©e WGS84
    atm INTEGER,                  -- Conditions m√©t√©o (1-9)
    year INTEGER,                 -- 2019-2024
    PRIMARY KEY (Num_Acc)
);

-- Tables sources (CSV bruts)
CREATE TABLE bronze.caracteristiques (...);  -- M√©tadonn√©es accident
CREATE TABLE bronze.usagers (...);           -- Victimes
```

#### ü•à Schema Silver

```sql
CREATE TABLE silver.features (
    -- Identifiant
    row_id INTEGER PRIMARY KEY,
    
    -- Target
    target INTEGER,               -- 0=pas accident, 1=accident
    
    -- Features g√©ographiques
    latitude DOUBLE,
    longitude DOUBLE,
    dist_to_noumea_km DOUBLE,
    accident_density_5km DOUBLE,
    
    -- Features temporelles
    hour INTEGER,                 -- 0-23
    dayofweek INTEGER,            -- 0-6
    month INTEGER,                -- 1-12
    atm INTEGER,                  -- 1-9
    hour_sin DOUBLE,              -- Encodage cyclique
    hour_cos DOUBLE,
    is_weekend BOOLEAN,
    is_rush_hour BOOLEAN,
    
    -- Features OSM
    road_type VARCHAR,
    speed_limit INTEGER,
    
    -- Features d'interaction
    lat_hour DOUBLE,
    lon_dayofweek DOUBLE,
    hour_dayofweek DOUBLE,
    -- ... (18 features d'interaction total)
);
```

#### ü•á Schema Gold

```sql
CREATE TABLE gold.train (
    -- M√™mes colonnes que silver.features
    -- 80% des donn√©es (split stratifi√©)
);

CREATE TABLE gold.test (
    -- M√™mes colonnes que silver.features
    -- 20% des donn√©es (split stratifi√©)
);

CREATE TABLE gold.feature_metadata (
    feature_name VARCHAR PRIMARY KEY,
    feature_order INTEGER,
    feature_type VARCHAR          -- 'numeric', 'categorical', 'interaction'
);
```

**Artefacts stock√©s en S3** :
- `s3://accidents-bucket/ducklake/artifacts/atm_encoder.pkl`
- `s3://accidents-bucket/ducklake/artifacts/features.pkl`
- `s3://accidents-bucket/ducklake/artifacts/accident_model.pkl` (si training)

### Features ML

**24 features enrichies** (6 de base + 18 calcul√©es) :

```python
# Features g√©ographiques de base (2)
features_base = ['latitude', 'longitude']

# Features temporelles de base (4)
features_temporelles = ['hour', 'dayofweek', 'month', 'atm']

# Interactions spatio-temporelles (12)
features_interactions = [
    'lat_hour', 'lon_hour',           # G√©ographie √ó heure
    'lat_dayofweek', 'lon_dayofweek', # G√©ographie √ó jour
    'lat_month', 'lon_month',         # G√©ographie √ó mois
    'hour_dayofweek', 'hour_month',   # Heure √ó jour/mois
    'dayofweek_month',                # Jour √ó mois
    'lat_lon', 'hour_dayofweek_month', 'lat_lon_hour'
]

# Encodage cyclique (4)
features_cycliques = [
    'hour_sin', 'hour_cos',           # Continuit√© 23h‚Üí0h
    'dayofweek_sin', 'dayofweek_cos'  # Continuit√© dimanche‚Üílundi
]

# Attributs OSM (2)
features_osm = ['road_type', 'speed_limit']

# M√©triques spatiales (2)
features_spatiales = [
    'accident_density_5km',    # Densit√© historique
    'nearest_accident_km'      # Distance accident le plus proche
]

# Indicateurs temporels avanc√©s (6)
features_temporelles_avancees = [
    'is_weekend',              # Samedi/Dimanche
    'is_rush_morning',         # 7h-9h
    'is_rush_evening',         # 17h-19h
    'is_night',                # 22h-6h
    'is_holiday',              # Jours f√©ri√©s NC
    'school_holidays'          # Vacances scolaires
]

# Distance aux centres urbains (1)
features_distance = ['dist_to_noumea_km']
```

---

## üîß D√©tails Techniques

### Strat√©gie d'√âchantillonnage N√©gatif

**Probl√®me** : Classifier route "normale" vs "accident" n√©cessite des contre-exemples r√©alistes.

**Solution** :

1. **Exclusion spatiale** :
   - Buffer 300m autour de chaque accident
   - Grille filtr√©e : ne garder que points ‚â•300m de tout accident historique
   - √âvite faux n√©gatifs (zones r√©ellement dangereuses)

2. **Distribution temporelle hybride** :
   - 85% : Timestamps √©chantillonn√©s depuis accidents r√©els (heures √† risque)
   - 15% : Timestamps uniformes sur p√©riode compl√®te (couvrir heures s√ªres)
   - √âquilibre entre ciblage et diversit√©

3. **Ratio r√©aliste** :
   - 22000:1 (refl√®te le taux d'accidents r√©el)
   - Calibration des probabilit√©s pour pr√©dictions r√©alistes
   - √âvite la sur-pr√©diction (√©tait 360√ó trop avec ratio 2:1)

### Importance des Features (Top 10 / 24 features)

| Feature | Importance | Interpr√©tation |
|---------|------------|----------------|
| `latitude` | 32.1% | Position g√©ographique principale |
| `longitude` | 24.8% | Zones urbaines vs rurales |
| `lat_lon` | 12.3% | Interaction g√©ographique |
| `dayofweek` | 8.7% | Week-end vs semaine |
| `road_type` | 6.2% | Type de route (OSM) |
| `hour_dayofweek` | 4.5% | Interaction temporelle |
| `dist_to_noumea_km` | 3.8% | Proximit√© centre urbain |
| `speed_limit` | 2.1% | Vitesse autoris√©e |
| `accident_density_5km` | 1.9% | Historique local |
| `hour_sin` | 1.6% | Cyclicit√© horaire |

**Insights** :
- **G√©ographie** : 69.2% (lat/lon/interactions) ‚Üí forte concentration spatiale
- **Features OSM** : 8.3% (road_type + speed_limit) ‚Üí gain significatif
- **Interactions** : 18.4% ‚Üí synergie spatio-temporelle captur√©e
- **14 features restantes** : 4.1% ‚Üí contribution marginale mais utile pour cas limites

### Comparaison d'Algorithmes (Optuna 50 trials)

| Mod√®le | Recall Accident | Precision | F1-Score | AUC | Temps Total (s) |
|--------|----------------|-----------|----------|-----|------------------|
| **CatBoost** ‚≠ê | **87.0%** | 97.9% | 0.92 | 0.974 | 74 |
| LightGBM | 86.2% | 97.6% | 0.91 | 0.972 | 58 |
| XGBoost | 84.8% | 97.1% | 0.90 | 0.968 | 92 |

**Configuration Optuna** :
- 50 essais par algorithme
- MedianPruner (arr√™t pr√©coce si performance < m√©diane)
- Optimisation m√©trique : Recall (classe minoritaire)
- Espace de recherche : learning_rate, max_depth, n_estimators, etc.

**S√©lection automatique** : CatBoost s√©lectionn√© (meilleur recall en 74s).

---

## üìä R√©sultats D√©taill√©s

### Matrice de Confusion

```
                Pr√©dit Non    Pr√©dit Oui
R√©el Non         2282 ‚úÖ        6 ‚ö†Ô∏è
R√©el Oui          40 ‚ùå       272 ‚úÖ
```

- **True Positives** : 272 accidents d√©tect√©s (87.0%)
- **False Negatives** : 40 accidents rat√©s (12.8%)
- **False Positives** : 6 fausses alarmes (0.26%)
- **True Negatives** : 2282 non-accidents corrects (99.7%)

### Courbe ROC

- **AUC-ROC** : 0.973
- Excellent compromis sensibilit√©/sp√©cificit√©

### Cas d'Usage

**Campagnes de pr√©vention** : Cibler les 10% de zones/heures les plus √† risque pour allouer 70% des ressources.

**Signalisation dynamique** : Afficher alertes en temps r√©el sur zones √† risque √©lev√© (‚â•80%).

**√âtude d'impact** : √âvaluer l'effet de nouvelles infrastructures sur le risque pr√©dit.

---

## üó∫Ô∏è Donn√©es Sources

### Accidents (data.gouv.fr)

- **Caract√©ristiques** : 6 CSV annuels (2019-2024)
- **Usagers** : D√©tails victimes
- **Filtrage** : `dep='988'` (Nouvelle-Cal√©donie)
- **Format dates** : `jour/mois/an hrmn` (fran√ßais)

### R√©seau Routier (OSM)

- **30+ communes** : Noum√©a, Dumb√©a, Mont-Dore, Pa√Øta, etc.
- **Type** : `network_type='drive'`
- **Cache** : `routes.nc` (GeoJSON, ~50MB)
- **Fallback** : Province Sud/Nord si commune √©choue

### Grille Spatiale

- **R√©solution** : 0.02¬∞ (~2.2km)
- **√âtendue** : lat ‚àà [-23.0, -19.5], lon ‚àà [163.5, 168.0]
- **Buffer routes** : 200m (EPSG:3857)
- **Points finaux** : ~1500 sur routes

---

## ÔøΩ D√©ploiement Kubernetes

### Configuration Initiale

1. **Cr√©er le Secret avec credentials** :

```bash
kubectl create secret generic rustfs-credentials-dagster -n dagster \
  --from-literal=AWS_ACCESS_KEY_ID=<your-key> \
  --from-literal=AWS_SECRET_ACCESS_KEY=<your-secret> \
  --from-literal=POSTGRES_PASSWORD=<pg-password>
```

2. **Cr√©er le ConfigMap** :

```bash
kubectl apply -f k8s/dagster/configmap-dagster-ducklake.yaml
```

3. **D√©ployer via ArgoCD** :

```bash
# Appliquer l'Application ArgoCD
kubectl apply -f k8s/dagster/dagster-helm-app-updated.yaml

# V√©rifier le sync
argocd app get dagster -n argocd
```

### Rebuild & Redeploy (D√©veloppement)

**Script automatis√©** : `rebuild-and-deploy.sh`

```bash
# √âditer le code
vim pipeline/stage_ingest.py

# Build + push + restart
./rebuild-and-deploy.sh

# R√©sultat:
# ‚úÖ Build Docker: ~45s
# ‚úÖ Push registry: ~20s  
# ‚úÖ Rollout restart: ~60s
# ‚úÖ Total: ~2min
```

**V√©rification** :

```bash
# Status des pods
kubectl get pods -n dagster | grep user-deployment

# Logs en temps r√©el
kubectl logs -f -n dagster deployment/dagster-user-deployment-accidents

# Tester connectivit√© DuckLake
kubectl exec -n dagster deployment/dagster-user-deployment-accidents -- \
  python -c "from pipeline.config import ensure_connection; conn = ensure_connection(); print('‚úÖ OK')"
```

### Troubleshooting

**Erreur : "No module named 'X'"**
‚Üí Ajouter la d√©pendance dans `pyproject.toml` puis rebuild

**Erreur : "fe_sendauth: no password supplied"**
‚Üí V√©rifier que `POSTGRES_PASSWORD` est dans le Secret

**Erreur : "Could not parse string '...' according to format"**
‚Üí V√©rifier le parsing datetime dans `pipeline/stage_ingest.py`

**Pod en CrashLoopBackOff**
‚Üí `kubectl logs -n dagster <pod-name>` pour voir l'erreur exacte

### Documentation Compl√®te

Voir [INDUSTRIALISATION.md](INDUSTRIALISATION.md) pour :
- Architecture d√©taill√©e Dagster + DuckLake
- Configuration ArgoCD
- Manifests Kubernetes
- Strat√©gies de d√©ploiement production

## ÔøΩüõ†Ô∏è D√©veloppement

### Structure du Code

**S√©paration des responsabilit√©s** :

```
pipeline/           ‚Üí Logique m√©tier (r√©utilisable)
‚îú‚îÄ‚îÄ config.py       ‚Üí Configuration DuckLake
‚îú‚îÄ‚îÄ stage_*.py      ‚Üí √âtapes du pipeline (ingest/features/datasets/modeling)
‚îî‚îÄ‚îÄ utils.py        ‚Üí Fonctions utilitaires

dagster_accidents/  ‚Üí Orchestration
‚îú‚îÄ‚îÄ assets_prod.py  ‚Üí Assets production (bronze/silver/gold)
‚îú‚îÄ‚îÄ assets.py       ‚Üí Assets complets (avec training)
‚îî‚îÄ‚îÄ repository.py   ‚Üí Definitions Dagster
```

**Principes** :
- `pipeline/` = source de v√©rit√© (ind√©pendant de Dagster)
- `dagster_accidents/` = wrappers minces (juste `@asset` + appels)
- Assets retournent des `dict` (m√©triques pour logs Dagster)

### Ajout d'un Nouvel Asset

1. **Cr√©er la fonction m√©tier** dans `pipeline/stage_<name>.py` :

```python
# pipeline/stage_predictions.py
from .config import ensure_connection

def generate_predictions():
    conn = ensure_connection()
    conn.execute("""
        CREATE OR REPLACE TABLE gold.predictions AS
        SELECT * FROM silver.features WHERE ...
    """)
    return {'predictions': 1500}
```

2. **Wrapper Dagster** dans `dagster_accidents/assets_prod.py` :

```python
from pipeline.stage_predictions import generate_predictions

@asset(
    name="gold_predictions",
    key_prefix=["gold"],
    deps=[gold_datasets],
    group_name="gold"
)
def gold_predictions(context: AssetExecutionContext) -> dict:
    stats = generate_predictions()
    context.log.info(f"[GOLD] Predictions: {stats}")
    return stats
```

3. **Rebuild & redeploy** :

```bash
./rebuild-and-deploy.sh
```

### Tests Locaux (sans K8s)

```bash
# Configurer DuckLake en local (fichier .env)
cat > .env << EOF
DUCKLAKE_DATABASE_URL=duckdb:///data/accidents_pipeline.duckdb
DUCKLAKE_DATA_PATH=./data/ducklake
EOF

# Tester une fonction pipeline directement
python -c "from pipeline.stage_ingest import ingest_all; print(ingest_all())"

# Ou lancer Dagster en local
uv run dagster dev -m dagster_accidents.repository -p 3000
```

### Structure Marimo (Notebooks Exploration)

Les notebooks Marimo utilisent une syntaxe r√©active :

```python
@app.cell
def _(dependencies):
    # Code ici
    return variables_export√©es
```

**R√®gles** :
- Variables uniques dans tout le notebook
- Derni√®re expression = output affich√©
- `mo.ui.*` pour √©l√©ments interactifs

**Usage recommand√©** : Exploration uniquement, pas pour la production.

---

## üìù Am√©liorations Futures

### Infrastructure & DevOps

- [x] Dagster sur Kubernetes ‚úÖ
- [x] Architecture m√©daillons (Bronze/Silver/Gold) ‚úÖ
- [x] DuckLake (PostgreSQL catalog + S3 storage) ‚úÖ
- [x] Script rebuild-and-deploy automatique ‚úÖ
- [ ] CI/CD avec GitLab CI / GitHub Actions
- [ ] Tests unitaires pipeline (pytest)
- [ ] Monitoring Prometheus + Grafana
- [ ] Alerting sur √©checs de mat√©rialisation

### Features G√©ospatiales Avanc√©es

- [x] Type de route (OSM : primary, secondary, residential) ‚úÖ
- [x] Vitesse limite ‚úÖ
- [x] Distance au centre urbain (Noum√©a) ‚úÖ
- [x] Densit√© d'accidents historiques (rayon 5km) ‚úÖ
- [ ] Pr√©sence d'intersections (rayon 100m)
- [ ] Courbure de la route
- [ ] Pente/d√©nivel√©

**Gain observ√©** : +0.5% recall (87.0% vs 86.5%)

### Features Temporelles

- [x] Jours f√©ri√©s (Nouvelle-Cal√©donie) ‚úÖ
- [x] Vacances scolaires ‚úÖ
- [x] Heures de pointe (matin/soir) ‚úÖ
- [x] Encodage cyclique (continuit√© temporelle) ‚úÖ
- [ ] √âv√©nements sp√©ciaux (festivals, matchs)
- [ ] Conditions m√©t√©o historiques (temp√©rature, pr√©cipitations)

### Mod√®les Alternatifs

- [x] Optuna hyperparameter tuning (CatBoost, LightGBM, XGBoost) ‚úÖ
- [ ] Stacking/Blending (ensemble des 3 meilleurs)
- [ ] TabNet (deep learning pour tabular)
- [ ] Mod√®les g√©ospatiaux (GWR, spatial lag)
- [ ] AutoML (AutoGluon, H2O)
- [ ] MLflow pour versioning et registry des mod√®les

### Applications

- [ ] API REST (FastAPI) avec pr√©dictions temps r√©el
- [ ] Dashboard Streamlit avec cartes interactives
- [ ] Notifications automatiques zones critiques
- [ ] Int√©gration avec syst√®mes de signalisation dynamique
- [ ] Scheduler Dagster pour pr√©dictions quotidiennes automatiques

---

## üìÑ Licence

Ce projet utilise des donn√©es publiques sous licence Open Data (data.gouv.fr) et OpenStreetMap (ODbL).

---

## üôè Remerciements

- **data.gouv.fr** : Donn√©es officielles accidents
- **OpenStreetMap** : R√©seau routier
- **Dagster** : Orchestration moderne des pipelines data
- **DuckDB** : Moteur SQL performant avec support DuckLake
- **Marimo** : Framework notebooks r√©actifs pour exploration

---

## üìß Contact

Pour questions, suggestions ou contributions, ouvrez une issue sur le d√©p√¥t GitHub.

**Derni√®re mise √† jour** : F√©vrier 2026 - Version Dagster/Kubernetes industrialis√©e